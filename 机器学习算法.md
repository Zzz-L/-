
---

<!-- TOC -->
- [逻辑回归分类器](#逻辑回归分类器)
- [支持向量机](#支持向量机)
- [朴素贝叶斯方法](#朴素贝叶斯方法)
- [决策树](#决策树)
- [集成学习](#集成学习)
  - [AdaBoost算法](#AdaBoost算法)
  - [GBDT算法](#GBDT算法)
  - [xgboost算法](#xgboost算法)
  - [bagging算法](#bagging算法)
  - [随机森林](#随机森林)
- [聚类算法](#聚类算法)
  - [kmeans](#kmeans)
  - [DBSCAN算法](#DBSCAN算法)
- [主成分分析PCA](#主成分分析PCA)
- [线性判别分析LDA](#线性判别分析LDA)
- 主成分、LDA、K-means、L1与L2正则化
- 模型评价方法
- 过拟合等实际问题如何解决

<!-- /TOC -->

## 逻辑回归分类器
1. 逻辑回归分类器是在线性回归的基础上构建，为了使得输入取值仅为0、1，因此采用了sigmoid函数映射
2. 逻辑回归模型：ln y/(1-y) = w*x+b
3. 逻辑回归得到P(Y=1|X) 和 P(Y=0|X)后，采用极大似然估计求解模型参数w、b
4. 极大似然估计思想：在样本已知的情况下，求解模型参数，使得样本出现的概率最大
   其中用似然函数表示样本出现的概率，因此极大似然估计即为使得似然函数最大化的参数
5. logistic回归的损失函数为对数损失   
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;L(w)&=-\log\left&space;(&space;\prod_{i=1}^N&space;[{\color{Red}&space;\sigma(x_i)}]^{{\color{Blue}&space;y_i}}&space;[{\color{Red}&space;1-&space;\sigma(x_i)}]^{{\color{Blue}&space;1-y_i}}&space;\right&space;)\\&space;&=-\sum_{i=1}^N&space;\left&space;[&space;y_i\log\sigma(x_i)&plus;(1-y_i)\log(1-\sigma(x_i))&space;\right&space;]\\&space;&=-\sum_{i=1}^N&space;\left&space;[&space;y_i\log\frac{\sigma(x_i)}{1-\sigma(x_i)}&plus;\log(1-\sigma(x_i))&space;\right&space;]&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;L(w)&=-\log\left&space;(&space;\prod_{i=1}^N&space;[{\color{Red}&space;\sigma(x_i)}]^{{\color{Blue}&space;y_i}}&space;[{\color{Red}&space;1-&space;\sigma(x_i)}]^{{\color{Blue}&space;1-y_i}}&space;\right&space;)\\&space;&=-\sum_{i=1}^N&space;\left&space;[&space;y_i\log\sigma(x_i)&plus;(1-y_i)\log(1-\sigma(x_i))&space;\right&space;]\\&space;&=-\sum_{i=1}^N&space;\left&space;[&space;y_i\log\frac{\sigma(x_i)}{1-\sigma(x_i)}&plus;\log(1-\sigma(x_i))&space;\right&space;]&space;\end{aligned}" title="\begin{aligned} L(w)&=-\log\left ( \prod_{i=1}^N [{\color{Red} \sigma(x_i)}]^{{\color{Blue} y_i}} [{\color{Red} 1- \sigma(x_i)}]^{{\color{Blue} 1-y_i}} \right )\\ &=-\sum_{i=1}^N \left [ y_i\log\sigma(x_i)+(1-y_i)\log(1-\sigma(x_i)) \right ]\\ &=-\sum_{i=1}^N \left [ y_i\log\frac{\sigma(x_i)}{1-\sigma(x_i)}+\log(1-\sigma(x_i)) \right ] \end{aligned}" /></a>
5. 参数优化: 针对上述损失函数，利用梯度下降法求解模型参数，梯度下降分为以下几步：
- 计算下降方向，即计算梯度
- 选择步长，更新参数（沿梯度负方向）
- 重复以上两步，直到两次迭代的差值小于某一阈值，则停止更新  
ps: 随机梯度下降与普通梯度下降的区别在于前者更新参数时只使用一个样本，而后者却使用所有训练样本
5. 多分类
- 多项式逻辑回归模型（类似于softmax回归）   

<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;P(Y=k|x)&=\frac{\exp(w_kx)}{1&plus;\sum_{k=1}^{K-1}&space;\exp(w_kx)}&space;\quad&space;k=1,2,..,K-1&space;\\&space;P(Y=K|x)&=\frac{1}{1&plus;\sum_{k=1}^{K-1}\exp(w_kx)}&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;P(Y=k|x)&=\frac{\exp(w_kx)}{1&plus;\sum_{k=1}^{K-1}&space;\exp(w_kx)}&space;\quad&space;k=1,2,..,K-1&space;\\&space;P(Y=K|x)&=\frac{1}{1&plus;\sum_{k=1}^{K-1}\exp(w_kx)}&space;\end{aligned}" title="\begin{aligned} P(Y=k|x)&=\frac{\exp(w_kx)}{1+\sum_{k=1}^{K-1} \exp(w_kx)} \quad k=1,2,..,K-1 \\ P(Y=K|x)&=\frac{1}{1+\sum_{k=1}^{K-1}\exp(w_kx)} \end{aligned}" /></a>
- 多分类逻辑回归采用[softmax回归](https://tech.meituan.com/intro_to_logistic_regression.html)   
- 也可采用构建多个分类器的方法，如one-vs-one、one-vs-all, 
  当类别之间互斥时，则需采用softmax回归
   

## 支持向量机
1. 基本思想：在特征空间中找到一个分割超平面，能够正确划分样本数据集，同时间隔最大化，间隔是指样本数据点到超平面的最短距离
2. 函数间隔：样本数据点（x,y）关于超平面 w*x + b的函数间隔定义为 <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{r}=w*x_i&plus;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{r}=w*x_i&plus;b" title="\hat{r}=w*x_i+b" /></a>   
   几何间隔：对w作规范化  
        <a href="https://www.codecogs.com/eqnedit.php?latex=r&space;=&space;\frac{w}{\left&space;\|&space;w&space;\right&space;\|}&space;*&space;x_i&space;&plus;&space;\frac{b}{\left&space;\|&space;w&space;\right&space;\|}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?r&space;=&space;\frac{w}{\left&space;\|&space;w&space;\right&space;\|}&space;*&space;x_i&space;&plus;&space;\frac{b}{\left&space;\|&space;w&space;\right&space;\|}" title="r = \frac{w}{\left \| w \right \|} * x_i + \frac{b}{\left \| w \right \|}" /></a>
3. 间隔最大化的目标函数  (线性可分)
- 最大化超平面对训练集的几何间隔r（所有样本点几何间隔的最小值）   
- 同时超平面关于每个样本点的几何间隔r 需满足约束条件，至少大于等于r   
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\max}}&space;\quad\gamma&space;\\&space;&\&space;\mathrm{s.t.}\quad,&space;y_i(\frac{w}{{\color{Red}&space;\left&space;|&space;w&space;\right&space;|}}x_i&plus;\frac{b}{{\color{Red}&space;\left&space;|&space;w&space;\right&space;|}})&space;\geq&space;\gamma,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\max}}&space;\quad\gamma&space;\\&space;&\&space;\mathrm{s.t.}\quad,&space;y_i(\frac{w}{{\color{Red}&space;\left&space;|&space;w&space;\right&space;|}}x_i&plus;\frac{b}{{\color{Red}&space;\left&space;|&space;w&space;\right&space;|}})&space;\geq&space;\gamma,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &{\color{Red} \underset{w,b}{\max}} \quad\gamma \\ &\ \mathrm{s.t.}\quad, y_i(\frac{w}{{\color{Red} \left | w \right |}}x_i+\frac{b}{{\color{Red} \left | w \right |}}) \geq \gamma,\quad i=1,2,\cdots,N \end{aligned}" /></a>
- 由于函数间隔取值对目标函数最大化没有影响，因此令函数间隔为1，最大化问题转化为最小化
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\min}&space;}&space;\&space;\frac{1}{2}{\color{Red}&space;\left&space;\|&space;w&space;\right&space;\|}&space;\\&space;&\&space;\mathrm{s.t.}\quad\,&space;y_i(wx_i&plus;b)&space;\geq&space;\&space;1,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\min}&space;}&space;\&space;\frac{1}{2}{\color{Red}&space;\left&space;\|&space;w&space;\right&space;\|}&space;\\&space;&\&space;\mathrm{s.t.}\quad\,&space;y_i(wx_i&plus;b)&space;\geq&space;\&space;1,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &{\color{Red} \underset{w,b}{\min} } \ \frac{1}{2}{\color{Red} \left \| w \right \|} \\ &\ \mathrm{s.t.}\quad\, y_i(wx_i+b) \geq \ 1,\quad i=1,2,\cdots,N \end{aligned}" /></a>
4.  针对上述凸二次规划问题，采用拉格朗日对偶算法求解
1. 构建**拉格朗日函数**     
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\\&space;&{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\\&space;&{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} L(w,b,{\color{Red} \alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\\ &{\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}" /></a>
1. 标准问题是求极小极大问题：    
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;{\color{Red}&space;\underset{w,b}{\min}}\&space;{\color{Blue}&space;\underset{\alpha}{\max}}\&space;L(w,b,\alpha)&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;{\color{Red}&space;\underset{w,b}{\min}}\&space;{\color{Blue}&space;\underset{\alpha}{\max}}\&space;L(w,b,\alpha)&space;\end{aligned}" title="\begin{aligned} {\color{Red} \underset{w,b}{\min}}\ {\color{Blue} \underset{\alpha}{\max}}\ L(w,b,\alpha) \end{aligned}" /></a>    
    其对偶问题为：    
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;{\color{Blue}&space;\underset{\alpha}{\max}}\&space;{\color{Red}&space;\underset{w,b}{\min}}\&space;L(w,b,\alpha)&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;{\color{Blue}&space;\underset{\alpha}{\max}}\&space;{\color{Red}&space;\underset{w,b}{\min}}\&space;L(w,b,\alpha)&space;\end{aligned}" title="\begin{aligned} {\color{Blue} \underset{\alpha}{\max}}\ {\color{Red} \underset{w,b}{\min}}\ L(w,b,\alpha) \end{aligned}" /></a>
1. 求 `L` 对 `(w,b)` 的极小    
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;w}=0&space;\;\;&\Rightarrow\;&space;w-\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}=0\\&space;&\Rightarrow\;&space;w=\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;w}=0&space;\;\;&\Rightarrow\;&space;w-\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}=0\\&space;&\Rightarrow\;&space;w=\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}&space;\end{aligned}" title="\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial w}=0 \;\;&\Rightarrow\; w-\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i}=0\\ &\Rightarrow\; w=\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i} \end{aligned}" /></a>   
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;b}=0&space;\;\;&\Rightarrow\;&space;\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i}=0&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;b}=0&space;\;\;&\Rightarrow\;&space;\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i}=0&space;\end{aligned}" title="\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial b}=0 \;\;&\Rightarrow\; \sum_{i=1}^N {\color{Red} \alpha_i y_i}=0 \end{aligned}" /></a>   
    结果代入`L`，有：   
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})&space;&=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\\&space;&=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N&space;\alpha_iy_ix_i-b\sum_{i=1}^N&space;\alpha_iy_i&plus;\sum_{i=1}^N&space;\alpha_i\\&space;&=\frac{1}{2}w^Tw-w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\\&space;&=-\frac{1}{2}w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\\&space;&=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;{\color{Red}&space;x_i^Tx_j}&plus;\sum_{i=1}^N&space;\alpha_i&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})&space;&=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\\&space;&=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N&space;\alpha_iy_ix_i-b\sum_{i=1}^N&space;\alpha_iy_i&plus;\sum_{i=1}^N&space;\alpha_i\\&space;&=\frac{1}{2}w^Tw-w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\\&space;&=-\frac{1}{2}w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\\&space;&=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;{\color{Red}&space;x_i^Tx_j}&plus;\sum_{i=1}^N&space;\alpha_i&space;\end{aligned}" title="\begin{aligned} L(w,b,{\color{Red} \alpha}) &=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\\ &=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N \alpha_iy_ix_i-b\sum_{i=1}^N \alpha_iy_i+\sum_{i=1}^N \alpha_i\\ &=\frac{1}{2}w^Tw-w^Tw+\sum_{i=1}^N \alpha_i\\ &=-\frac{1}{2}w^Tw+\sum_{i=1}^N \alpha_i\\ &=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot {\color{Red} x_i^Tx_j}+\sum_{i=1}^N \alpha_i \end{aligned}" /></a>  
1. 求 `L` 对 `α` 的极大，即   
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&\underset{\alpha}{\max}&space;\quad&space;-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j&plus;\sum_{i=1}^N&space;\alpha_i\\&space;&\&space;\mathrm{s.t.}\quad\;&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,\&space;\&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;&\underset{\alpha}{\max}&space;\quad&space;-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j&plus;\sum_{i=1}^N&space;\alpha_i\\&space;&\&space;\mathrm{s.t.}\quad\;&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,\&space;\&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &\underset{\alpha}{\max} \quad -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot x_i^Tx_j+\sum_{i=1}^N \alpha_i\\ &\ \mathrm{s.t.}\quad\; \sum_{i=1}^N \alpha_i y_i=0,\ \ {\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}" /></a>
    该问题的对偶问题为：   
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&{\color{Red}&space;\underset{\alpha}{\min}&space;}&space;\quad\&space;\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j-\sum_{i=1}^N&space;\alpha_i\\&space;&\&space;\mathrm{s.t.}\quad\;&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,\&space;\&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;&{\color{Red}&space;\underset{\alpha}{\min}&space;}&space;\quad\&space;\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j-\sum_{i=1}^N&space;\alpha_i\\&space;&\&space;\mathrm{s.t.}\quad\;&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,\&space;\&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned})" title="\begin{aligned} &{\color{Red} \underset{\alpha}{\min} } \quad\ \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot x_i^Tx_j-\sum_{i=1}^N \alpha_i\\ &\ \mathrm{s.t.}\quad\; \sum_{i=1}^N \alpha_i y_i=0,\ \ {\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned})" /></a>    
    于是，标准问题最后等价于求解该**对偶问题**    
    > 继续求解该优化问题，有 [SMO 方法](https://blog.csdn.net/ajianyingxiaoqinghan/article/details/73087304#t11)；因为《统计学习方法》也只讨论到这里，故推导也止于此   

5. SVM的优缺点
- 优点：具有鲁棒性，因为只有少数支持向量决定模型结果，因此模型对异常值不敏感，增删样本对模型没有影响
- 对于在现有维度线性不可分的数据，可以利用核函数映射到高维使其线性可分
- 缺点：对大规模训练样本难以实施，因为svm求解参数的过程为二次规划问题，涉及矩阵计算，当样本量过大时，矩阵计算量太大
- 对于多分类问题，没有较好的解决方案
- 针对非线性可分数据，核函数的选取较难（主要靠经验）
## 朴素贝叶斯方法
1. 基本思想：分类方法，根据条件独立性假设学习训练数据的联合概率分布，对于预测样本x，  
   结合贝叶斯定理，计算后验概率P(Y|x), 选择后验概率最大的类作为输入x的所属类别
2. 条件独立性假设是指在类别给定的情况，各分类特征条件独立
3. 贝叶斯分类器：y=argmax(P(Y=k|X=x))
4. 损失函数为0-1损失时，期望风险最小化等价于后验概率最大化
5. 贝叶斯分类器待估计参数：先验概率p(Y=k)，条件概率p(X=x|Y=k)  
   采用极大似然估计或者贝叶斯估计（后者是防止极大似然估计所估计的概率为0，因此添加常数项）
6. 贝叶斯估计因为学习了训练样本的联合概率分布，因此属于生成式模型

## 决策树
1. 决策树是一种分类与回归方法，通过构建一颗二叉树或多叉树实现分类、回归，
   包括特征选择、决策树生成、决策树剪枝
2. 三种算法：ID3算法、C4.5算法、cart算法
- ID3算法：1) 首先计算每个特征的信息增益，选择信息增益最大的特征划分数据集  
             - 信息增益表示给定特征后，对类的信息的不确定性减少的程度（比如区分化妆与否，性别与学历两个特征的信息增益不同）   
             - 信息增益的计算g(D,A) = H(D) - H(D|A) （划分前熵-划分后熵）   
             - H(D)表示原始数据集D的熵     
             <a href="http://www.codecogs.com/eqnedit.php?latex=H(D)&space;=&space;-\sum_{i=1}^{n}P_i&space;log(P_i)" target="_blank"><img src="http://latex.codecogs.com/gif.latex?H(D)&space;=&space;-\sum_{i=1}^{n}P_i&space;log(P_i)" title="H(D) = -\sum_{i=1}^{n}P_i log(P_i)" /></a>    
             - H(D|A)表示给定特征A后的条件熵    
             <a href="http://www.codecogs.com/eqnedit.php?latex=H(D|A)&space;=&space;-\sum_{i=1}^{n}P_i&space;H(D|A=a_i)&space;\\&space;P_i&space;=&space;P(A=a_i)" target="_blank"><img src="http://latex.codecogs.com/gif.latex?H(D|A)&space;=&space;-\sum_{i=1}^{n}P_i&space;H(D|A=a_i)&space;\\&space;P_i&space;=&space;P(A=a_i)" title="H(D|A) = -\sum_{i=1}^{n}P_i H(D|A=a_i) \\ P_i = P(A=a_i)" /></a>    
          2) 划分后的数据集若类别不同，则需在剩下的特征中寻找最大的特征继续划分数据集  
          3）直到所有特征用完或者每个分支下的样本属于同一类，则停止划分，若所有特征用完，  
             但样本类别不统一，则采用投票的方法决定所属类别     
  总结：ID3算法只使用于离散变量，同时只能用于分类，不能用于回归   
 - C4.5算法：与ID3算法的区别在于使用信息增益比选择特征，同时适用于连续变量
             1）使用信息增益比，信息增益/划分前熵，因为信息增益偏向选择取值较多的特征
             2）处理连续特征，对连续特征取值排序，任意两个值选取中间值划分数据集，尝试每种划分方法，
                然后计算划分前后的信息熵，选择信息增益最大的分裂点作为该特征的分裂点
- cart算法：通过构建二叉分类树或回归树，用于分类时，使用基尼系数选择特征，而用于回归时，则使用平方误差最小选择特征以及分割点   
1）回归二叉树   
- y为连续值，将输入x划分为m个区域，每个区域的取值为C1,C2,,,CM，因此最小二乘回归树模型为    
<a href="http://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;\sum_{m=1}^{M}C_m&space;I(x\in&space;R_m)" target="_blank"><img src="http://latex.codecogs.com/gif.latex?f(x)&space;=&space;\sum_{m=1}^{M}C_m&space;I(x\in&space;R_m)" title="f(x) = \sum_{m=1}^{M}C_m I(x\in R_m)" /></a>
- 利用平方误差最小化，寻找最优划分的特征以及分裂点，优化目标函数如下：   
<a href="http://www.codecogs.com/eqnedit.php?latex=\underset{j,s}{min}[\underset{C_1}{min}\sum_{x_i\in&space;R_1(j,s)}(y_i-C_1)^{2}&plus;\underset{C_2}{min}\sum_{x_i\in&space;R_2(j,s)}(y_i-C_2)^{2}]" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\underset{j,s}{min}[\underset{C_1}{min}\sum_{x_i\in&space;R_1(j,s)}(y_i-C_1)^{2}&plus;\underset{C_2}{min}\sum_{x_i\in&space;R_2(j,s)}(y_i-C_2)^{2}]" title="\underset{j,s}{min}[\underset{C_1}{min}\sum_{x_i\in R_1(j,s)}(y_i-C_1)^{2}+\underset{C_2}{min}\sum_{x_i\in R_2(j,s)}(y_i-C_2)^{2}]" /></a>
- 依次遍历所有特征j，以及每个特征的每个取值s，选择使得损失函数最小的（j,s）   
2) 分类二叉树
- 使用基尼指数寻找划分特征以及最优划分点，基尼指数表示集合的不确定性，基尼指数越大，则数据的不纯度越高   
<a href="http://www.codecogs.com/eqnedit.php?latex=Gini(p)&space;=&space;\sum_{k=1}^{K}P_k(1-P_k)&space;=1-&space;\sum_{k=1}^{K}P_k^{2}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?Gini(p)&space;=&space;\sum_{k=1}^{K}P_k(1-P_k)&space;=1-&space;\sum_{k=1}^{K}P_k^{2}" title="Gini(p) = \sum_{k=1}^{K}P_k(1-P_k) =1- \sum_{k=1}^{K}P_k^{2}" /></a>

## 集成学习
1. 集成学习是指构建并结合多个分类器来完成学习任务，其中多个分类器可以是同质分类器，此时被称为基分类器，比如决策树集成，神经网络集成，
   集成也可包含不同分类器，如同时包含决策树和神经网络，称为组件学习器
- 集成学习为什么有效？：平均上，集成学习至少能与多分类器中一个保持一致的分类效果，而当多个分类器的误差独立时，集成学习的效果比其成员效果更好
- **集成学习的核心：个体学习器要有一定的准确率，同时分类器具有差异，即实现“好而不同”**
2. 集成学习分两类：boosting:各分类器之间相互依赖，必须串行生成的序列方法;   
   bagging: 各分类器之间不存在强依赖关系，可以并行生成
3. boosting: adaboost、GBDT、xgboost   （减少偏差）
   - boosting需解决两个问题：第一，每一轮如何改变数据的权值以及概率分布；    
                            第二，如何组合多个弱分类器
4. bagging: 随机森林（减少方差）

### AdaBoost算法
1. 含义: 对样本赋予初始权重1/N，基于训练数据构建基分类器，根据分类器的错误率调整样本权重，
   然后继续构建分类器，直到达到指定T个，然后对T个分类器加权求和
2. 针对boosting的两个问题：
- 初始对每个样本赋予相同的权值，建立分类器后，提高上一轮分类错误的样本权值，降低分类正确的样本权值
- 采用加法模型，加大分类错误率低的模型的权重，使其在表决中起更大的作用
3. adaboost模型为加法模型，损失函数为指数损失，同时学习算法是前向分步算法的二分类学习方法       
   损失函数为指数函数L(y,f(x)) = exp(-y*f(x))    
   前向分步算法思想：从前向后，每一步只学习一个基函数及其系数，逐步优化目标函数   
#### 算法描述
- 输入：训练集 T={(x1,y1),..,(xN,yN)}, xi ∈ R^n, yi ∈ {-1,+1}，基学习器 G1(x)
- 输出：最终学习器 G(x)
1. 初始化样本权值   
<a href="http://www.codecogs.com/eqnedit.php?latex=D_1=(w_{1,1},\cdots,w_{1,i},\cdots,w_{1,N}),\quad&space;w_{1,i}=\frac{1}{N},\quad&space;i=1,2,\cdots,N" target="_blank"><img src="http://latex.codecogs.com/gif.latex?D_1=(w_{1,1},\cdots,w_{1,i},\cdots,w_{1,N}),\quad&space;w_{1,i}=\frac{1}{N},\quad&space;i=1,2,\cdots,N" title="D_1=(w_{1,1},\cdots,w_{1,i},\cdots,w_{1,N}),\quad w_{1,i}=\frac{1}{N},\quad i=1,2,\cdots,N" /></a>
2. 构建基分类器   
<a href="http://www.codecogs.com/eqnedit.php?latex=G_m(x):\chi&space;\rightarrow&space;{-1,&plus;1}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?G_m(x):\chi&space;\rightarrow&space;{-1,&plus;1}" title="G_m(x):\chi \rightarrow {-1,+1}" /></a>
3. 计算基分类器的分类错误率，即为分类错误样本的权值之和   
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;e_m&=P(G_m(x_i)\neq&space;y_i)\\&=\sum_{i=1}^Nw_{m,i}\cdot&space;{\color{Red}&space;I(G_m(x_i)\neq&space;y_i)}&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;e_m&=P(G_m(x_i)\neq&space;y_i)\\&=\sum_{i=1}^Nw_{m,i}\cdot&space;{\color{Red}&space;I(G_m(x_i)\neq&space;y_i)}&space;\end{aligned}" title="\begin{aligned} e_m&=P(G_m(x_i)\neq y_i)\\&=\sum_{i=1}^Nw_{m,i}\cdot {\color{Red} I(G_m(x_i)\neq y_i)} \end{aligned}" /></a>
4. 计算分类器的权重   
<a href="http://www.codecogs.com/eqnedit.php?latex=\alpha_m=\frac{1}{2}\ln\frac{1-e_m}{e_m}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\alpha_m=\frac{1}{2}\ln\frac{1-e_m}{e_m}" title="\alpha_m=\frac{1}{2}\ln\frac{1-e_m}{e_m}" /></a>
5. 更新样本的权值，分类错误与分类正确的样本权值相差exp(2a)  
<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;D_{{\color{Red}m&plus;1}}&=(w_{m&plus;1,1},\cdots,w_{m&plus;1,i},\cdots,w_{m&plus;1,N})\\w_{{\color{Red}m&plus;1},i}&=\frac{w_{{\color{Red}m},i}\cdot\exp(-\alpha_{\color{Red}m}\cdot{\color{Blue}y_iG_m(x_i)&space;})}{Z_{\color{Red}m}}&space;\end{aligned}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{aligned}&space;D_{{\color{Red}m&plus;1}}&=(w_{m&plus;1,1},\cdots,w_{m&plus;1,i},\cdots,w_{m&plus;1,N})\\w_{{\color{Red}m&plus;1},i}&=\frac{w_{{\color{Red}m},i}\cdot\exp(-\alpha_{\color{Red}m}\cdot{\color{Blue}y_iG_m(x_i)&space;})}{Z_{\color{Red}m}}&space;\end{aligned}" title="\begin{aligned} D_{{\color{Red}m+1}}&=(w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N})\\w_{{\color{Red}m+1},i}&=\frac{w_{{\color{Red}m},i}\cdot\exp(-\alpha_{\color{Red}m}\cdot{\color{Blue}y_iG_m(x_i) })}{Z_{\color{Red}m}} \end{aligned}" /></a>
6. 重复构建m个分类器，最终线性组合，得到最终集成分类器   
<a href="http://www.codecogs.com/eqnedit.php?latex=G(x)=\mathrm{sign}(\sum_{m=1}^M\alpha_mG_m(x))" target="_blank"><img src="http://latex.codecogs.com/gif.latex?G(x)=\mathrm{sign}(\sum_{m=1}^M\alpha_mG_m(x))" title="G(x)=\mathrm{sign}(\sum_{m=1}^M\alpha_mG_m(x))" /></a>  
**总结：AdaBoost不改变训练数据的分布，但通过改变训练数据的权值分布，使得训练数据在不同分类器中起到不同的作用

### GBDT算法
1. 提升树：迭代生成多个模型，将每个模型的预测结果相加，后面模型基于前面模型的效果生成   
          在回归问题中，新的树是通过不断拟合残差得到，累加所有树的结果作为最终结果    
2. 梯度提升：当损失函数为平方损失或指数损失时，残差易得到，但一般的损失函数，不易得到残差时，则利用**损失函数的负梯度作为残差的近似值**
3. GBDT：以CART回归树为基学习器的梯度提升算法, 每棵树的深度较小
3. GBDT算法描述   
输入：训练集 T={(x1,y1),..,(xN,yN)}, xi ∈ R^n, yi ∈ R；损失函数 L(y,f(x))；   
输出：回归树 f_M(x)  
- 初始化回归树
<a href="http://www.codecogs.com/eqnedit.php?latex=f_0(x)={\color{Red}&space;\arg\underset{c}{\min}}\sum_{i=1}^NL(y_i,c)" target="_blank"><img src="http://latex.codecogs.com/gif.latex?f_0(x)={\color{Red}&space;\arg\underset{c}{\min}}\sum_{i=1}^NL(y_i,c)" title="f_0(x)={\color{Red} \arg\underset{c}{\min}}\sum_{i=1}^NL(y_i,c)" /></a>   
- 对于每一轮，计算残差/负梯度   
<a href="http://www.codecogs.com/eqnedit.php?latex=r_{m,i}=-\frac{\partial&space;L(y_i,{\color{Red}&space;f_{m-1}(x_i)}))}{\partial&space;{\color{Red}&space;f_{m-1}(x_i)}}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?r_{m,i}=-\frac{\partial&space;L(y_i,{\color{Red}&space;f_{m-1}(x_i)}))}{\partial&space;{\color{Red}&space;f_{m-1}(x_i)}}" title="r_{m,i}=-\frac{\partial L(y_i,{\color{Red} f_{m-1}(x_i)}))}{\partial {\color{Red} f_{m-1}(x_i)}}" /></a>
- 对负梯度拟合一个回归树，得到第m棵树的叶结点区域   
<a href="http://www.codecogs.com/eqnedit.php?latex=R_{m,j},\quad&space;j=1,2,..,J" target="_blank"><img src="http://latex.codecogs.com/gif.latex?R_{m,j},\quad&space;j=1,2,..,J" title="R_{m,j},\quad j=1,2,..,J" /></a>
- 估计每个叶结点区域的值   
<a href="http://www.codecogs.com/eqnedit.php?latex=c_{m,j}={\color{Red}&space;\arg\underset{c}{\min}}\sum_{x_i\in&space;R_{m,j}}L(y_i,{\color{Blue}&space;f_{m-1}(x_i)&plus;c})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?c_{m,j}={\color{Red}&space;\arg\underset{c}{\min}}\sum_{x_i\in&space;R_{m,j}}L(y_i,{\color{Blue}&space;f_{m-1}(x_i)&plus;c})" title="c_{m,j}={\color{Red} \arg\underset{c}{\min}}\sum_{x_i\in R_{m,j}}L(y_i,{\color{Blue} f_{m-1}(x_i)+c})" /></a>
- 更新回归树   
<a href="http://www.codecogs.com/eqnedit.php?latex=f_m(x)=f_{m-1}&plus;\sum_{j=1}^J&space;c_{m,j}{\color{Blue}&space;I(x\in&space;R_{m,j})}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?f_m(x)=f_{m-1}&plus;\sum_{j=1}^J&space;c_{m,j}{\color{Blue}&space;I(x\in&space;R_{m,j})}" title="f_m(x)=f_{m-1}+\sum_{j=1}^J c_{m,j}{\color{Blue} I(x\in R_{m,j})}" /></a>   

### xgboost算法
1. xgboost算法是GBDT的高效实现，基分类器除了CART回归树，还可以是线性分类器
2. 损失函数添加了正则化项，包括 L2 权重衰减和对叶子数的限制
3. 使用牛顿法代替梯度下降法寻找最优解，前者使用一阶+二阶导数作为残差，后者只使用了一阶导数
4. 传统 CART树寻找最优切分点的标准是最小化均方差；
   XGBoost 通过最大化得分公式来寻找最优切分点：  
   这同时也起到了“剪枝”的作用——如果分数小于γ，则不会增加分支；  
   <a href="http://www.codecogs.com/eqnedit.php?latex=Gain&space;=&space;\frac{1}{2}&space;\left[\frac{G_L^2}{H_L&plus;\lambda}&plus;\frac{G_R^2}{H_R&plus;\lambda}-\frac{(G_L&plus;G_R)^2}{H_L&plus;H_R&plus;\lambda}\right]&space;-&space;\gamma" target="_blank"><img src="http://latex.codecogs.com/gif.latex?Gain&space;=&space;\frac{1}{2}&space;\left[\frac{G_L^2}{H_L&plus;\lambda}&plus;\frac{G_R^2}{H_R&plus;\lambda}-\frac{(G_L&plus;G_R)^2}{H_L&plus;H_R&plus;\lambda}\right]&space;-&space;\gamma" title="Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma" /></a>

### bagging算法
1. 提升集成学习的效果，就需使得个体学习器之间独立，而bagging通过对样本进行自助采样，
   使得基分类器的训练样本不同，让基分类器之间产生较大差异，从而提升预测效果
2. 流程：采样出T个含有m个样本的训练集，然后基于每个训练集构建基分类器，最后将这些基学习器组合，
   通常对于分类，采用简单投票法，而对于回归，则采用简单平均法
3. 优点：高效，与训练基学习器的复杂度同阶; 与AdaBoost只能用于二分类不同，bagging可以用于多分类或者回归；  
   由于自助采样，有部分样本并未进入任何一个训练集，因此可以作为验证集测试模型泛化性能
   
### 随机森林
1. 在以决策树为基分类器构建bagging的基础上，每棵决策树随机选取部分特征（通过boostrap的方法随机选择k个特征子集），每棵树的深度要求较高
2. 随机森林基学习器的多样性不仅来源于样本扰动，还来源于属性扰动，使得最终集成的泛化性能由于个体学习器之间的差异度增加而进一步提升
3. 优点：训练效率优于bagging，因为只采用了部分特征，基分类器的性能


## 聚类算法
### kmeans
1. 算法过程：
- 随机选择聚类初始中心点
- 计算各样本点到中心的距离（一般是欧式距离），并将各类划分到距离最近的类
- 重新计算类中心（对类的样本点各维度平均）
- 重复以上两个步骤，直到聚类中心不发生变化
2. kmeans划分族的标准为最小化平方误差(误差平方和也作为聚类性能的评价标准)

<a href="http://www.codecogs.com/eqnedit.php?latex=E&space;=&space;\sum_{i=1}^{k}\sum_{x\in&space;C_i}\left&space;\|&space;x-u_i&space;\right&space;\|^{2}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?E&space;=&space;\sum_{i=1}^{k}\sum_{x\in&space;C_i}\left&space;\|&space;x-u_i&space;\right&space;\|^{2}" title="E = \sum_{i=1}^{k}\sum_{x\in C_i}\left \| x-u_i \right \|^{2}" /></a>

3. 优点与缺点
- 优点：便于实现   
- 缺点：k值需要事先定义，对随机初始化的中心点敏感，容易收敛到局部最小值，只适用于球型数据，同时只适用于数值型数据

### DBSCAN算法
1. dbscan是基于密度的聚类算法，基于密度的聚类算法适用于非球状数据
2. dbscan将样本观测值分为三类：核心点、边界点、噪音点
- 核心点：样本点的e-领域的样本点个数大于MinPats
- 边界点：e-领域的样本点个数小于MinPats,但又在核心点的领域内
- 噪音点：e-领域的样本点个数小于MinPats,同时不在核心点的领域内
3. dbscan对样本点之间关系的定义
- 密度直达：若样本点q在核心点P的e-领域内，则q由样本点p密度直达（不满足对称性）
- 密度可达：存在核心点序列p1,p2,...p(n-1)之间密度直达，则p(n)(无论是否为核心点)由p1密度可达（不满足对称性）
- 密度相连：存在样本点O,使得p和q分别由O密度可达，则称p和q密度相连
4. 算法原理：dbscan认为紧密相连的样本应归为一类，即对于任意核心点，找到其密度可达的样本生成聚类簇，直到所有核心点均访问晚
4. 算法过程：首先计算所有样本点e-领域内的样本点个数，找出核心点集合，然后随机选出一个核心点P，
            并将它的e-领域点归为类A，然后在e-领域点中找到其他核心点q，并将q的e-领域中的点也合并为类A，
            因为这些点由P点密度可达
5. 优点与缺点
- 优点：不需要事先定义类的个数，对聚类初始中心点不敏感，可以使用于任何形状的数据集，可以识别异常值
- 缺点：参数调优e、Minpts较为复杂，需要对两者联合调参；
        当样本点密度不均，聚类间距相差很大，聚类质量很差时，不适用
       数据点较多时，收敛较慢


### 主成分分析PCA
1. 基本思想：通过将高维样本点映射到低维空间，同时又希望投影后的投影值尽可能分散（最大可分性）
2. 投影值尽可能分散，转化为投影后的样本点方差最大化，因此为了便于计算，将所有样本中心化
- 单变量投影后的方差   
![images](https://github.com/Zzz-L/campus-recruitment-notes/blob/master/images/PCA1.png)
- 多变量投影后的协方差矩阵  
![images](https://github.com/Zzz-L/campus-recruitment-notes/blob/master/images/PCA2.png)
3. 目标函数为最大化协方差矩阵，同时投影向量的模为1      
![images](https://github.com/Zzz-L/campus-recruitment-notes/blob/master/images/PCA3.png)   
利用拉格朗日乘子法求解   
![images](https://github.com/Zzz-L/campus-recruitment-notes/blob/master/images/PCA4.png)    
![images](https://github.com/Zzz-L/campus-recruitment-notes/blob/master/images/PCA5.png)    
![images](https://github.com/Zzz-L/campus-recruitment-notes/blob/master/images/PCA6.png)  
因此最终转化为求映射前的协方差矩阵C的特征值与特征向量，特征向量即为对应的映射向量，而特征值即为对应特征向量的方差，    
根据特征值大小对特征向量排序，取前K个向量构成投影矩阵
4. 算法过程：
- 样本零均值化（为了更好求方差）
- 计算样本协方差矩阵<a href="http://www.codecogs.com/eqnedit.php?latex=C&space;=&space;\frac{1}{m}XX^{t}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?C&space;=&space;\frac{1}{m}XX^{t}" title="C = \frac{1}{m}XX^{t}" /></a>
- 对协方差矩阵特征值分解，得到特征值与特征向量（高维时采用奇异值分解）
- 根据特征值对特征向量排序，取前K个特征向量构成投影矩阵P
- Y=PX 即为降维到K维后的数据
5. 核化线性降维
普通PCA从高维空间到低维空间是线性映射，而某些情况，只有非线性映射才能找到恰当的低维嵌入，   
因此采用核函数先将样本映射到高维空间，再在特征空间实施PCA

### 线性判别分析LDA
1. 也是常用的降维方法，与PCA的区别在于，LDA使用了类别信息，而PCA是完全无监督的 
   PCA是为了让映射后的样本具有最大的发散性，而LDA是为了让映射后的样本具有最大的分类性能
2. 基本思想：对样本点映射到某一平面，使得类与类之间间隔尽量远，而类内间隔尽量近
3. LDA降维流程：
- 二分类情况下，将样本点投影到y = wx 直线上，两类样本点的中心点在直线上的映射为wu0、wu1  
   投影后，两类样本点的协方差为<a href="http://www.codecogs.com/eqnedit.php?latex=w^{t}\Sigma&space;_{0}w&space;\&space;\&space;w^{t}\Sigma&space;_{1}w" target="_blank"><img src="http://latex.codecogs.com/gif.latex?w^{t}\Sigma&space;_{0}w&space;\&space;\&space;w^{t}\Sigma&space;_{1}w" title="w^{t}\Sigma _{0}w \ \ w^{t}\Sigma _{1}w" /></a>
- 欲使同类样本的投影点尽可能接近：即<a href="http://www.codecogs.com/eqnedit.php?latex=w^{t}\Sigma&space;_{0}w&plus;w^{t}\Sigma&space;_{1}w" target="_blank"><img src="http://latex.codecogs.com/gif.latex?w^{t}\Sigma&space;_{0}w&plus;w^{t}\Sigma&space;_{1}w" title="w^{t}\Sigma _{0}w+w^{t}\Sigma _{1}w" /></a> 尽可能小    
- 欲使异类样本点的投影尽可能远，即<a href="http://www.codecogs.com/eqnedit.php?latex=\left&space;\|&space;w^{t}u_0-&space;w^{t}u_1\right&space;\|^{2}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\left&space;\|&space;w^{t}u_0-&space;w^{t}u_1\right&space;\|^{2}" title="\left \| w^{t}u_0- w^{t}u_1\right \|^{2}" /></a> 尽可能大
- 最终的目标函数为
<a href="http://www.codecogs.com/eqnedit.php?latex=J&space;=&space;\frac{\left&space;\|&space;w^{t}u_0-&space;w^{t}u_1\right&space;\|^{2}}{w^{t}\Sigma&space;_{0}w&plus;w^{t}\Sigma&space;_{1}w}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?J&space;=&space;\frac{\left&space;\|&space;w^{t}u_0-&space;w^{t}u_1\right&space;\|^{2}}{w^{t}\Sigma&space;_{0}w&plus;w^{t}\Sigma&space;_{1}w}" title="J = \frac{\left \| w^{t}u_0- w^{t}u_1\right \|^{2}}{w^{t}\Sigma _{0}w+w^{t}\Sigma _{1}w}" /></a>
